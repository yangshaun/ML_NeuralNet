{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "[5234, 3081, 12, 6, 195, 2, 3134]\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66, 56, 95, 57, 45, 14, 41,  8, 49, 26, 16, 88, 15, 12, 85, 61])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 1000000\n",
    "\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "valid_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1171, 3], [868, 7882], [7174, 6995], [3647, 2636], [6818, 5126], [1772, 7891], [41, 1], [2688, 1922], [2411, 885], [2244, 55]] [1, 1, 0, 0, 0, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding/Identity:0\", shape=(None, 1, 300), dtype=float32)\n",
      "Tensor(\"embedding_1/Identity:0\", shape=(None, 1, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "print (target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "print (context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "# similarity = concatenate([target, context], mode='cos', dot_axes=0)\n",
    "from keras.layers import dot\n",
    "\n",
    "similarity = dot([target, context], axes=1, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = dot([target, context], axes=1, normalize=True)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model([input_target, input_context],output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_model = Model([input_target, input_context],similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6237655282020569\n",
      "Nearest to these: ants, ally, resolution, fiction, strongly, evangelical, bavaria, tiny,\n",
      "Nearest to many: entitled, alaska, mary, wagner, remainder, terminals, differential, displacement,\n",
      "Nearest to so: tends, shell, merit, methods, era, friendly, spencer, condition,\n",
      "Nearest to who: bwv, primitive, trial, parade, popularly, qualified, gradually, reliable,\n",
      "Nearest to its: hadith, impose, allegedly, pbs, ambrose, necessary, join, neighbours,\n",
      "Nearest to for: frank, franklin, tribal, memories, writes, classify, yahoo, outcome,\n",
      "Nearest to has: battles, pp, portland, chemicals, stone, video, dolphin, anger,\n",
      "Nearest to zero: northwestern, precious, pronunciation, treatment, theorists, monopoly, weight, gardens,\n",
      "Nearest to had: codex, floating, unification, boys, franklin, richard, identification, indonesian,\n",
      "Nearest to are: synonym, goths, james, telling, experiments, ants, addresses, indonesia,\n",
      "Nearest to five: domains, quest, beautiful, emperors, asimov, couple, jackson, tubes,\n",
      "Nearest to if: iowa, councils, vancouver, bytes, perfect, christ, mk, moisture,\n",
      "Nearest to s: dakota, implied, marxist, compiler, garbage, blair, aspect, align,\n",
      "Nearest to as: galileo, subsidiary, surnames, t, graphite, triumph, kinetic, cocaine,\n",
      "Nearest to during: historian, habsburg, zeus, thereafter, upon, poets, occasions, select,\n",
      "Nearest to after: mainz, spinal, predict, liberation, stating, railways, abbreviated, december,\n",
      "Iteration 100, loss=0.756510317325592\n",
      "Iteration 200, loss=0.6705676317214966\n",
      "Iteration 300, loss=0.6788266897201538\n",
      "Iteration 400, loss=0.686737596988678\n",
      "Iteration 500, loss=0.6356461048126221\n",
      "Iteration 600, loss=0.7392033338546753\n",
      "Iteration 700, loss=0.6523776650428772\n",
      "Iteration 800, loss=0.5116875171661377\n",
      "Iteration 900, loss=0.7546942234039307\n",
      "Iteration 1000, loss=0.714867353439331\n",
      "Iteration 1100, loss=0.676889181137085\n",
      "Iteration 1200, loss=0.7727828025817871\n",
      "Iteration 1300, loss=0.7166628241539001\n",
      "Iteration 1400, loss=0.7393044829368591\n",
      "Iteration 1500, loss=0.746070921421051\n",
      "Iteration 1600, loss=0.6147772669792175\n",
      "Iteration 1700, loss=0.6490623354911804\n",
      "Iteration 1800, loss=0.6186323165893555\n",
      "Iteration 1900, loss=0.579606831073761\n",
      "Iteration 2000, loss=0.7544155716896057\n",
      "Iteration 2100, loss=0.6544415354728699\n",
      "Iteration 2200, loss=0.7564730048179626\n",
      "Iteration 2300, loss=0.7031881809234619\n",
      "Iteration 2400, loss=0.6639559268951416\n",
      "Iteration 2500, loss=0.5264254212379456\n",
      "Iteration 2600, loss=0.6723838448524475\n",
      "Iteration 2700, loss=0.643480122089386\n",
      "Iteration 2800, loss=0.6686781644821167\n",
      "Iteration 2900, loss=0.6824547648429871\n",
      "Iteration 3000, loss=0.7862870097160339\n",
      "Iteration 3100, loss=0.738221287727356\n",
      "Iteration 3200, loss=0.6199080944061279\n",
      "Iteration 3300, loss=0.6797223091125488\n",
      "Iteration 3400, loss=0.6602001190185547\n",
      "Iteration 3500, loss=0.6817506551742554\n",
      "Iteration 3600, loss=0.7274718284606934\n",
      "Iteration 3700, loss=0.6858991384506226\n",
      "Iteration 3800, loss=0.7343506217002869\n",
      "Iteration 3900, loss=0.794369101524353\n",
      "Iteration 4000, loss=0.6862748265266418\n",
      "Iteration 4100, loss=0.5909243226051331\n",
      "Iteration 4200, loss=0.7381716370582581\n",
      "Iteration 4300, loss=0.6576257348060608\n",
      "Iteration 4400, loss=0.6611036658287048\n",
      "Iteration 4500, loss=0.7523560523986816\n",
      "Iteration 4600, loss=0.6674374938011169\n",
      "Iteration 4700, loss=0.6270774006843567\n",
      "Iteration 4800, loss=0.7623100280761719\n",
      "Iteration 4900, loss=0.653843879699707\n",
      "Iteration 5000, loss=0.697499692440033\n",
      "Iteration 5100, loss=0.6252683401107788\n",
      "Iteration 5200, loss=0.7610389590263367\n",
      "Iteration 5300, loss=0.6698691248893738\n",
      "Iteration 5400, loss=0.7899868488311768\n",
      "Iteration 5500, loss=0.6673757433891296\n",
      "Iteration 5600, loss=0.712365984916687\n",
      "Iteration 5700, loss=0.6906687021255493\n",
      "Iteration 5800, loss=0.7300350069999695\n",
      "Iteration 5900, loss=0.6903500556945801\n",
      "Iteration 6000, loss=0.7397314310073853\n",
      "Iteration 6100, loss=0.7938540577888489\n",
      "Iteration 6200, loss=0.7266504168510437\n",
      "Iteration 6300, loss=0.7634335160255432\n",
      "Iteration 6400, loss=0.772139310836792\n",
      "Iteration 6500, loss=0.6387874484062195\n",
      "Iteration 6600, loss=0.6927708387374878\n",
      "Iteration 6700, loss=0.7531956434249878\n",
      "Iteration 6800, loss=0.6786897778511047\n",
      "Iteration 6900, loss=0.643328845500946\n",
      "Iteration 7000, loss=0.6780999898910522\n",
      "Iteration 7100, loss=0.7356221079826355\n",
      "Iteration 7200, loss=0.7699891328811646\n",
      "Iteration 7300, loss=0.7066696286201477\n",
      "Iteration 7400, loss=0.6984744668006897\n",
      "Iteration 7500, loss=0.7264730334281921\n",
      "Iteration 7600, loss=0.7038513422012329\n",
      "Iteration 7700, loss=0.7609671354293823\n",
      "Iteration 7800, loss=0.7396864295005798\n",
      "Iteration 7900, loss=0.655185878276825\n",
      "Iteration 8000, loss=0.7486118674278259\n",
      "Iteration 8100, loss=0.6726880073547363\n",
      "Iteration 8200, loss=0.6800339221954346\n",
      "Iteration 8300, loss=0.7136114835739136\n",
      "Iteration 8400, loss=0.7838326692581177\n",
      "Iteration 8500, loss=0.668776273727417\n",
      "Iteration 8600, loss=0.7170319557189941\n",
      "Iteration 8700, loss=0.6967238783836365\n",
      "Iteration 8800, loss=0.7553181052207947\n",
      "Iteration 8900, loss=0.7245952486991882\n",
      "Iteration 9000, loss=0.7508846521377563\n",
      "Iteration 9100, loss=0.640326976776123\n",
      "Iteration 9200, loss=0.737257719039917\n",
      "Iteration 9300, loss=0.6591129899024963\n",
      "Iteration 9400, loss=0.742262065410614\n",
      "Iteration 9500, loss=0.6431005597114563\n",
      "Iteration 9600, loss=1.5161771774291992\n",
      "Iteration 9700, loss=0.628740668296814\n",
      "Iteration 9800, loss=0.7478654980659485\n",
      "Iteration 9900, loss=0.7127305269241333\n",
      "Iteration 10000, loss=0.6930601000785828\n",
      "Nearest to these: ants, jewish, resolution, ois, ally, tiny, revelation, aesthetic,\n",
      "Nearest to many: displacement, wagner, independently, entitled, remainder, alaska, leave, assumes,\n",
      "Nearest to so: june, friendly, shell, runner, though, spencer, array, condition,\n",
      "Nearest to who: trial, bwv, primitive, parade, qualified, observatory, stockholm, mrna,\n",
      "Nearest to its: hadith, belief, coke, pbs, allegedly, raymond, identity, join,\n",
      "Nearest to for: judgment, confederation, supply, frank, yahoo, suez, writes, tamil,\n",
      "Nearest to has: folk, bengal, discussion, celebrations, pp, proud, portland, scripture,\n",
      "Nearest to zero: pronunciation, nevertheless, harder, intellectual, eighth, melting, monopoly, organize,\n",
      "Nearest to had: boys, unification, useful, elements, codex, buying, units, floating,\n",
      "Nearest to are: gram, interrupt, synonym, challenges, mercedes, around, email, organism,\n",
      "Nearest to five: asimov, offices, indicator, purchased, studio, girl, greeks, pilot,\n",
      "Nearest to if: resembles, mk, iowa, production, specialist, bankruptcy, vectors, acronym,\n",
      "Nearest to s: inputs, intended, blair, align, launch, viewed, croatian, grounds,\n",
      "Nearest to as: texture, graphite, person, reflection, surnames, fa, hausdorff, attached,\n",
      "Nearest to during: historian, occasions, telegraph, majesty, incorporate, poets, ness, thereafter,\n",
      "Nearest to after: liberation, mainz, colleagues, railways, spinal, georg, consequences, december,\n",
      "Iteration 10100, loss=0.7258389592170715\n",
      "Iteration 10200, loss=0.6666184067726135\n",
      "Iteration 10300, loss=0.7036217451095581\n",
      "Iteration 10400, loss=0.7117488384246826\n",
      "Iteration 10500, loss=0.7152658104896545\n",
      "Iteration 10600, loss=0.668046772480011\n",
      "Iteration 10700, loss=0.6958874464035034\n",
      "Iteration 10800, loss=0.7326796054840088\n",
      "Iteration 10900, loss=0.6908146739006042\n",
      "Iteration 11000, loss=0.7220632433891296\n",
      "Iteration 11100, loss=0.7188529968261719\n",
      "Iteration 11200, loss=0.6764985918998718\n",
      "Iteration 11300, loss=0.7594377994537354\n",
      "Iteration 11400, loss=0.7803888916969299\n",
      "Iteration 11500, loss=0.7075385451316833\n",
      "Iteration 11600, loss=0.6263530254364014\n",
      "Iteration 11700, loss=0.6896973848342896\n",
      "Iteration 11800, loss=0.7407028079032898\n",
      "Iteration 11900, loss=0.7034017443656921\n",
      "Iteration 12000, loss=0.6664955615997314\n",
      "Iteration 12100, loss=0.6948384046554565\n",
      "Iteration 12200, loss=0.7127776741981506\n",
      "Iteration 12300, loss=0.6945687532424927\n",
      "Iteration 12400, loss=0.6046326160430908\n",
      "Iteration 12500, loss=0.4950965642929077\n",
      "Iteration 12600, loss=0.67231285572052\n",
      "Iteration 12700, loss=0.7129336595535278\n",
      "Iteration 12800, loss=0.7375364303588867\n",
      "Iteration 12900, loss=0.6709077954292297\n",
      "Iteration 13000, loss=0.6957319974899292\n",
      "Iteration 13100, loss=0.7064169645309448\n",
      "Iteration 13200, loss=0.7549023628234863\n",
      "Iteration 13300, loss=0.706305205821991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13400, loss=0.7108515501022339\n",
      "Iteration 13500, loss=0.6933519840240479\n",
      "Iteration 13600, loss=0.6776277422904968\n",
      "Iteration 13700, loss=0.6722545027732849\n",
      "Iteration 13800, loss=0.7045323252677917\n",
      "Iteration 13900, loss=0.6901738047599792\n",
      "Iteration 14000, loss=0.6697689890861511\n",
      "Iteration 14100, loss=0.6435079574584961\n",
      "Iteration 14200, loss=0.6717710494995117\n",
      "Iteration 14300, loss=0.7310406565666199\n",
      "Iteration 14400, loss=0.6139265298843384\n",
      "Iteration 14500, loss=0.6975826025009155\n",
      "Iteration 14600, loss=0.7188666462898254\n",
      "Iteration 14700, loss=0.7240556478500366\n",
      "Iteration 14800, loss=0.7065849304199219\n",
      "Iteration 14900, loss=0.6840465068817139\n",
      "Iteration 15000, loss=0.6966611742973328\n",
      "Iteration 15100, loss=0.6271365880966187\n",
      "Iteration 15200, loss=0.7349530458450317\n",
      "Iteration 15300, loss=0.678281843662262\n",
      "Iteration 15400, loss=0.7253926992416382\n",
      "Iteration 15500, loss=0.6726604700088501\n",
      "Iteration 15600, loss=0.686711847782135\n",
      "Iteration 15700, loss=0.7136297821998596\n",
      "Iteration 15800, loss=0.7001503705978394\n",
      "Iteration 15900, loss=0.7371503114700317\n",
      "Iteration 16000, loss=0.692960262298584\n",
      "Iteration 16100, loss=0.6232743263244629\n",
      "Iteration 16200, loss=0.6776564717292786\n",
      "Iteration 16300, loss=0.7349954843521118\n",
      "Iteration 16400, loss=0.6560239791870117\n",
      "Iteration 16500, loss=0.6833751797676086\n",
      "Iteration 16600, loss=0.6898152232170105\n",
      "Iteration 16700, loss=0.6823185682296753\n",
      "Iteration 16800, loss=0.7361719012260437\n",
      "Iteration 16900, loss=0.6248347759246826\n",
      "Iteration 17000, loss=0.7267670631408691\n",
      "Iteration 17100, loss=0.7970407009124756\n",
      "Iteration 17200, loss=0.7414563298225403\n",
      "Iteration 17300, loss=0.5979531407356262\n",
      "Iteration 17400, loss=0.6850311756134033\n",
      "Iteration 17500, loss=0.680834174156189\n",
      "Iteration 17600, loss=0.6423218250274658\n",
      "Iteration 17700, loss=0.668558657169342\n",
      "Iteration 17800, loss=0.7050904035568237\n",
      "Iteration 17900, loss=0.6727026700973511\n",
      "Iteration 18000, loss=0.6443495750427246\n",
      "Iteration 18100, loss=0.7171928882598877\n",
      "Iteration 18200, loss=0.7377937436103821\n",
      "Iteration 18300, loss=0.7185502052307129\n",
      "Iteration 18400, loss=0.7664235830307007\n",
      "Iteration 18500, loss=0.6664429306983948\n",
      "Iteration 18600, loss=0.7146984934806824\n",
      "Iteration 18700, loss=0.6762414574623108\n",
      "Iteration 18800, loss=0.6870293617248535\n",
      "Iteration 18900, loss=0.6235764026641846\n",
      "Iteration 19000, loss=0.6918215751647949\n",
      "Iteration 19100, loss=0.7192867994308472\n",
      "Iteration 19200, loss=0.672971248626709\n",
      "Iteration 19300, loss=0.6096971035003662\n",
      "Iteration 19400, loss=0.684603214263916\n",
      "Iteration 19500, loss=0.656981885433197\n",
      "Iteration 19600, loss=0.7100809812545776\n",
      "Iteration 19700, loss=0.7545912265777588\n",
      "Iteration 19800, loss=0.693139374256134\n",
      "Iteration 19900, loss=0.638733983039856\n",
      "Iteration 20000, loss=0.6353127956390381\n",
      "Nearest to these: ants, jewish, resolution, successors, shallow, acts, aesthetic, publicity,\n",
      "Nearest to many: newfoundland, leave, alaska, independently, entitled, went, linux, battalion,\n",
      "Nearest to so: vertical, garden, condition, air, june, though, spencer, friendly,\n",
      "Nearest to who: bwv, trial, varies, primitive, parade, versus, voters, vol,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6fcf39786d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration {}, loss={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msim_cb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-6fcf39786d7b>\u001b[0m in \u001b[0;36mrun_sim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mvalid_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m  \u001b[0;31m# number of nearest neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mnearest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlog_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Nearest to %s:'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalid_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6fcf39786d7b>\u001b[0m in \u001b[0;36m_get_sim\u001b[0;34m(valid_word_idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0min_arr2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_arr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_arr2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1434\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict_on_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m       \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_batch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1437\u001b[0m       \u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \"\"\"\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    593\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         dataset = _OptimizeDataset(dataset, graph_rewrites,\n\u001b[0;32m--> 375\u001b[0;31m                                    graph_rewrite_configs)\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;31m# (3) Apply autotune options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations, optimization_configs)\u001b[0m\n\u001b[1;32m   4363\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4364\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4365\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4366\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset\u001b[0;34m(input_dataset, optimizations, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3678\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimization_configs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3679\u001b[0;31m         optimization_configs)\n\u001b[0m\u001b[1;32m   3680\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3681\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()\n",
    "\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
