{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pprint as pp\n",
    "ds,info = tfds.load('mnist', split='train', shuffle_files=True,with_info=True)\n",
    "train_images, train_labels = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split='train', \n",
    "    batch_size=-1, \n",
    "    as_supervised=True,\n",
    "))\n",
    "\n",
    "train_images = (train_images / 255)\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "\n",
    "depth = 10\n",
    "train_labels = tf.one_hot(train_labels, depth).numpy()\n",
    "\n",
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X,Y, batch_size):\n",
    "    index = np.random.randint(0, len(X), batch_size)\n",
    "    return X[index, :], Y[index]\n",
    "num_batches = int(len(train_images) // batch_size * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, input_size, label_size, activation, num_layers=6,\n",
    "                 hidden_size=10):\n",
    "        self._input_size = input_size\n",
    "        self._label_size = label_size\n",
    "        self._activation = activation\n",
    "        # num layers does not include the input layer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._model_def()\n",
    "\n",
    "    def _model_def(self):\n",
    "        \n",
    "        self.input_images = tf.placeholder(tf.float32, shape=[None, self._input_size])\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[None, self._label_size])\n",
    "        \n",
    "        input = self.input_images\n",
    "        for i in range(self._num_layers - 1):\n",
    "            input = tf.layers.dense(input, self._hidden_size, activation=self._activation,name='layer{}'.format(i+1))\n",
    "            \n",
    "            \n",
    "        logits = tf.layers.dense(input, 10, name='layer{}'.format(self._num_layers))\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=self.labels))\n",
    "\n",
    "        \n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "        self._log_gradients(self._num_layers)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "        self.accuracy = self._compute_accuracy(logits, self.labels)\n",
    "        tf.summary.scalar('acc', self.accuracy)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def _compute_accuracy(self, logits, labels):\n",
    "        prediction = tf.argmax(logits, 1)\n",
    "        equality = tf.equal(prediction, tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def _log_gradients(self, num_layers):\n",
    "        gr = tf.get_default_graph()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            weight = gr.get_tensor_by_name('layer{}/kernel:0'.format(i + 1))\n",
    "            grad = tf.gradients(self.loss, weight)[0]\n",
    "            mean = tf.reduce_mean(tf.abs(grad))\n",
    "            tf.summary.scalar('mean_{}'.format(i + 1), mean)\n",
    "            tf.summary.histogram('histogram_{}'.format(i + 1), grad)\n",
    "            tf.summary.histogram('hist_weights_{}'.format(i + 1), grad)\n",
    "base_path = \"./Tensorboard/\"           \n",
    "def run_training(model, X,Y, sub_folder, iterations=3333, batch_size=30):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(model.init_op)\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(base_path + sub_folder,sess.graph)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            image_batch, label_batch = get_batch(X,Y,batch_size)\n",
    "            l, _, acc = sess.run([model.loss, model.optimizer, model.accuracy],\n",
    "                                 feed_dict={model.input_images: image_batch, model.labels: label_batch})\n",
    "            if i % 200 == 0:\n",
    "                summary = sess.run(model.merged, feed_dict={model.input_images: image_batch,\n",
    "                                                            model.labels: label_batch})\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print(\"Iteration {} of {}, loss: {:.3f}, train accuracy: \"\n",
    "                      \"{:.2f}%\".format(i, iterations, l, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0627 19:58:52.733618 4451820992 deprecation.py:323] From <ipython-input-4-d1fbd16f6ecc>:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "W0627 19:58:52.742202 4451820992 deprecation.py:323] From /Users/shaunyang/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0627 19:58:52.763722 4451820992 deprecation.py:506] From /Users/shaunyang/anaconda3/envs/tensorflow_ENV/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running scenario: sigmoid\n",
      "Iteration 0 of 3333, loss: 2.321, train accuracy: 6.67%\n",
      "Iteration 200 of 3333, loss: 2.307, train accuracy: 6.67%\n",
      "Iteration 400 of 3333, loss: 2.289, train accuracy: 16.67%\n",
      "Iteration 600 of 3333, loss: 2.300, train accuracy: 10.00%\n",
      "Iteration 800 of 3333, loss: 2.283, train accuracy: 16.67%\n",
      "Iteration 1000 of 3333, loss: 2.291, train accuracy: 16.67%\n",
      "Iteration 1200 of 3333, loss: 2.323, train accuracy: 3.33%\n",
      "Iteration 1400 of 3333, loss: 2.297, train accuracy: 13.33%\n",
      "Iteration 1600 of 3333, loss: 2.331, train accuracy: 13.33%\n",
      "Iteration 1800 of 3333, loss: 2.319, train accuracy: 3.33%\n",
      "Iteration 2000 of 3333, loss: 2.295, train accuracy: 16.67%\n",
      "Iteration 2200 of 3333, loss: 2.313, train accuracy: 10.00%\n",
      "Iteration 2400 of 3333, loss: 2.301, train accuracy: 16.67%\n",
      "Iteration 2600 of 3333, loss: 2.309, train accuracy: 6.67%\n",
      "Iteration 2800 of 3333, loss: 2.304, train accuracy: 6.67%\n",
      "Iteration 3000 of 3333, loss: 2.302, train accuracy: 10.00%\n",
      "Iteration 3200 of 3333, loss: 2.311, train accuracy: 6.67%\n",
      "Running scenario: relu\n",
      "Iteration 0 of 3333, loss: 2.303, train accuracy: 10.00%\n",
      "Iteration 200 of 3333, loss: 1.939, train accuracy: 23.33%\n",
      "Iteration 400 of 3333, loss: 1.822, train accuracy: 30.00%\n",
      "Iteration 600 of 3333, loss: 1.409, train accuracy: 40.00%\n",
      "Iteration 800 of 3333, loss: 1.320, train accuracy: 36.67%\n",
      "Iteration 1000 of 3333, loss: 1.027, train accuracy: 53.33%\n",
      "Iteration 1200 of 3333, loss: 0.986, train accuracy: 60.00%\n",
      "Iteration 1400 of 3333, loss: 1.126, train accuracy: 33.33%\n",
      "Iteration 1600 of 3333, loss: 1.400, train accuracy: 53.33%\n",
      "Iteration 1800 of 3333, loss: 0.933, train accuracy: 63.33%\n",
      "Iteration 2000 of 3333, loss: 1.402, train accuracy: 60.00%\n",
      "Iteration 2200 of 3333, loss: 1.238, train accuracy: 56.67%\n",
      "Iteration 2400 of 3333, loss: 0.852, train accuracy: 70.00%\n",
      "Iteration 2600 of 3333, loss: 1.436, train accuracy: 56.67%\n",
      "Iteration 2800 of 3333, loss: 1.021, train accuracy: 56.67%\n",
      "Iteration 3000 of 3333, loss: 0.802, train accuracy: 80.00%\n",
      "Iteration 3200 of 3333, loss: 0.913, train accuracy: 70.00%\n",
      "Running scenario: leaky_relu\n",
      "Iteration 0 of 3333, loss: 2.303, train accuracy: 13.33%\n",
      "Iteration 200 of 3333, loss: 1.985, train accuracy: 16.67%\n",
      "Iteration 400 of 3333, loss: 1.713, train accuracy: 23.33%\n",
      "Iteration 600 of 3333, loss: 1.257, train accuracy: 53.33%\n",
      "Iteration 800 of 3333, loss: 1.360, train accuracy: 53.33%\n",
      "Iteration 1000 of 3333, loss: 1.110, train accuracy: 73.33%\n",
      "Iteration 1200 of 3333, loss: 0.974, train accuracy: 66.67%\n",
      "Iteration 1400 of 3333, loss: 1.169, train accuracy: 46.67%\n",
      "Iteration 1600 of 3333, loss: 1.097, train accuracy: 46.67%\n",
      "Iteration 1800 of 3333, loss: 0.760, train accuracy: 70.00%\n",
      "Iteration 2000 of 3333, loss: 0.888, train accuracy: 70.00%\n",
      "Iteration 2200 of 3333, loss: 0.654, train accuracy: 80.00%\n",
      "Iteration 2400 of 3333, loss: 0.829, train accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "scenarios = [\"sigmoid\", \"relu\", \"leaky_relu\"]\n",
    "act_funcs = [tf.sigmoid, tf.nn.relu, tf.nn.leaky_relu]\n",
    "\n",
    "X = train_images\n",
    "Y = train_labels\n",
    "\n",
    "for i in range(len(scenarios)):\n",
    "    tf.reset_default_graph()\n",
    "    print(\"Running scenario: {}\".format(scenarios[i]))\n",
    "    model = Model(784, 10, act_funcs[i], 20, 10)\n",
    "    run_training(model, X,Y, scenarios[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"./Tensorboard\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
